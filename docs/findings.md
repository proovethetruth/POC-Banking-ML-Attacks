
# Предисловие
Проект делался в не самых лучших условиях — с температурой — но как раз это помогло быстро проверить главное: насколько уязвима обычная ML-модель кредитного скоринга, если её просто выкатить в прод без защиты. Без доступа к коду модели удалось реализовать три разных атаки: на решение (HopSkipJump), на извлечение модели через ответы, и inference по принадлежности к train/test. Пришлось немного повозиться с адаптацией ART к табличным данным — он явно больше заточен под картинки — и добиться воспроизводимых атак. Больше боя с документацией и параметрами, чем инженерии. Но итог очевидный: даже такая простая модель оказывается уязвимой, если забить на безопасность

## 1. Decision-based Adversarial Attack (HopSkipJump)

### Суть атаки  
Атака работает без доступа к градиентам модели по единственному сигналу "правильно/неправильно" (decision boundary) подбирая мин. изменения входного вектора, которые переводят предсказание в нужный класс или разрушают его качество  

### Сложность  
Надо находить опорные точки на границе решений и затем итеративно улучшать "нарушение", при этом каждый запрос к модели стоит дорого и сам процесс требует сотен–тысяч оценок

### Когда доступна 
- Не введён rate-limiting или капча на API, поэтому можно спамить запросами  
- Модель принимает любые вектора без фильтрации аномалий или отклонений от тренировочного распределения
- Модель не обучалась на атакующих примерах и не обладает устойчивостью к таким искажениям
- Нет системы обнаружения аномалий (кучи некорректных запросов) - атаку не заметят

### Реализация
1. Применил clip_values на основе статистики train, чтобы обходить неожиданные выходы за допустимые диапазоны
2. Сгенерировал начальные «шумовые» вектора в пределах допустимых отклонений, чтобы найти опорные точки на decision boundary
3. Подобрал max_iter, init_eval, init_size для баланса "число запросов <-> качество perturbation"
4. Фиксировал random seed, чтобы результаты атаки можно было легко обсуждать и сравнивать

Даже без градиентной информации можно эффективно сломать табличную модель на продакшн-уровне. Подбор параметров и воспроизводимости — главные вызовы, но при грамотной настройке HopSkipJump становится надёжным инструментом для аудита ML-систем

## 2. Model Extraction Attack

### Суть атаки  
Используя доступ к предсказаниям модели (input -> output), обучаем суррогатную модель, повторяющую поведение оригинала, обходя ограничения и раскрывая внутреннюю логику

### Сложность  
Требуется собрать достаточное количество разнообразных запросов, чтобы покрыть пространство признаков, и управлять балансом между объёмом запросов и качеством суррогатной модели

### Когда доступна  
- Нет ограничения на число запросов к API или отсутствует rate-limiting  
- Модель возвращает вероятности классов или сырой логит, а не только метку  
- Нет output perturbation или differential privacy, поэтому ответы точны и детерминированы  
- Поля признаков стандартизированы и не меняют порядок/имена, что упрощает сбор обучающей выборки для surrogate

### Реализация
1. Сохранил X_test и получил `victim.predict_proba(X_test)` и `victim.predict(X_test)`  
2. Обучил LogisticRegression на X_test -> метки victim, чтобы получить surrogate с высокой fidelity  
3. Оценил fidelity (~95 %) и accuracy surrogate vs ground truth (~81.7 %)  
4. Сравнил отчёты classification_report

С помощью surrogate-модели можно обойти платные или защищённые API, получить легковесную копию скоринговой системы и проводить дальнейшие атаки или анализ без доступа к коду оригинала

## 3. Membership Inference Attack

### Суть атаки  
По поведению модели на входе определяем, был ли сэмпл в тренировочном наборе по разнице в confidence между известными и новыми образцами  

### Сложность  
Нужно выбрать подходящий показатель уверенности и корректно выставить threshold, учитывая дисбаланс классов и нелинейность распределения  

### Когда доступна  
- Модель возвращает вероятности классов или confidence score  
- Нет output perturbation или differential privacy  
- Нет ограничения на число inference-запросов  
- Нет мониторинга подозрительных паттернов 

### Реализация
1. Вычислил `conf_train = max(proba)` для X_train и `conf_test = max(proba)` для X_test  
2. Установил `threshold = median(conf_train)`  
3. Отнёс sample к "member", если его confidence > threshold  
4. Оценил Attack Accuracy (~ 55.6 %) и ROC AUC (~ 72.4 %)  

Даже простейший threshold-based подход показывает достаточно высокий AUC, что требует введения защит — prediction clipping или differential privacy